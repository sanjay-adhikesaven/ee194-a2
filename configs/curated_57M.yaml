# Curated: 57.6M params, ~328M tokens (319K docs), 3 epochs
# Quality-filtered data, no synthetic augmentation

experiment_id: curated_57M

model:
  vocab_size: 16384
  hidden_size: 576
  num_layers: 10
  num_attention_heads: 8
  num_kv_heads: 2
  intermediate_size: 2304
  max_position_embeddings: 2048
  rms_norm_eps: 1.0e-5
  rope_theta: 10000.0
  initializer_range: 0.02
  tie_word_embeddings: true

train:
  data_path: /home/jason/ee194-a2/partb_mixture_quality_filtered_data
  output_dir: runs/curated_57M
  tokenizer_dir: tokenizer_16k
  batch_size: 32
  gradient_accumulation_steps: 2
  learning_rate: 3.0e-4
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-8
  max_grad_norm: 1.0
  warmup_fraction: 0.05
  num_epochs: 3
  dtype: bfloat16
  compile_model: true
  log_interval: 50
  save_interval_epochs: 1
  wandb_project: ee194-a2-pretrain
  wandb_run_name: curated_57M
  seed: 42
