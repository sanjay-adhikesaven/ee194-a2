{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8a22a",
   "metadata": {},
   "source": [
    "# NeMo Curator Lab\n",
    "\n",
    "**Assignment 2, Part a** | UC Berkeley EE 194/290-16: Scalable AI | Spring 2026\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this assignment, students will use NeMo Curator to download and curate Wikipedia data within a chosen domain. Then, they will compare the non-curated data with the curated data and reason about how data curation may impact downstream tasks.\n",
    "\n",
    "This assignment will be scored out of 70 points, with an opportunity to earn an additional 5 bonus points during the evaluation section. Refer to the headings for breakdowns of the scoring rubric.\n",
    "\n",
    "**Deliverable**: Submit this notebook with all cells implemented and run, including each output per cell.\n",
    "\n",
    "## Environment Setup [0 points]\n",
    "\n",
    "Refer to Curator's [Installation Guide](https://docs.nvidia.com/nemo/curator/latest/admin/installation.html) to install and run Curator via Docker or `uv`. If using `uv`, include the `text_cuda12` extra (or install `all` which includes `text_cuda12`). Verify the environment and library setups with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifies curator is installed\n",
    "import nemo_curator\n",
    "\n",
    "print(nemo_curator.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifies that gpu dependencies are installed\n",
    "import cudf\n",
    "\n",
    "print(cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08394be6",
   "metadata": {},
   "source": [
    "Initialize and start a Ray client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962701a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.core.client import RayClient\n",
    "\n",
    "ray_client = RayClient(num_cpus=32)\n",
    "ray_client.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d2ff0",
   "metadata": {},
   "source": [
    "Note: If you encounter out of memory errors while downloading the data, then you may need to stop the Ray client (with `ray_client.stop()`), reduce `num_cpus` above, and start a fresh Ray client.\n",
    "\n",
    "## Data Setup [5 points]\n",
    "\n",
    "Use the following code to download and extract data from the latest Wikipedia dump. Limit it to English text only.\n",
    "\n",
    "You are encouraged to experiment with the URL limit and record limit. Some helpful information:\n",
    "- There are ~70 URLs per dump\n",
    "- The number of JSONL files written == `url_limit`\n",
    "- There are ~20,000 records per URL\n",
    "- The number of rows per JSONL file == `record_limit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ebf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.text.download.wikipedia.stage import WikipediaDownloadExtractStage\n",
    "\n",
    "language = \"en\"  # do not change this\n",
    "download_dir = \"./wiki_downloads\"\n",
    "url_limit = 2  # experiment with this\n",
    "record_limit = 100  # experiment with this\n",
    "\n",
    "# Initialize the Wikipedia download stage\n",
    "wiki_stage = WikipediaDownloadExtractStage(\n",
    "    language=language,\n",
    "    download_dir=download_dir,\n",
    "    url_limit=url_limit,\n",
    "    record_limit=record_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.text.io.writer.jsonl import JsonlWriter\n",
    "\n",
    "wiki_data_dir = \"./wiki_data\"\n",
    "\n",
    "# Initialize the JSONL writer stage\n",
    "jsonl_writer = JsonlWriter(wiki_data_dir, write_kwargs={\"force_ascii\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.pipeline import Pipeline\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = Pipeline(\"download_wiki_pipeline\")\n",
    "\n",
    "# Add the stages to the pipeline\n",
    "pipeline.add_stage(wiki_stage)\n",
    "pipeline.add_stage(jsonl_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27065889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f277cc2",
   "metadata": {},
   "source": [
    "## Implement a Stage [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d77e7",
   "metadata": {},
   "source": [
    "Use the code snippet below to read and inspect a portion of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nemo_curator.utils.file_utils import get_all_file_paths_under\n",
    "\n",
    "# Read and inspect the first JSONL file from wiki_data_dir\n",
    "file_paths = get_all_file_paths_under(wiki_data_dir)\n",
    "df = pd.read_json(file_paths[0], lines=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6f2c5",
   "metadata": {},
   "source": [
    "In the Wikipedia dataset, the last portion of the document's text typically corresponds \"Categories\" section of the Wikipedia page. For example, the text for the Wikipedia page on [Anarchism](https://en.wikipedia.org/wiki/Anarchism) looks like:\n",
    "\n",
    "```text\n",
    "Anarchism is a political philosophy and movement... \\n\\nExternal links \\n\\n Anarchy Archives â€“ an online research center on the history and theory of anarchism.\\n\\n \\nAnti-capitalism\\nAnti-fascism\\nEconomic ideologies\\nFar-left politics\\nLeft-wing ideologies\\nLibertarian socialism\\nLibertarianism\\nPolitical culture\\nPolitical ideologies\\nPolitical movements\\nSocial theories\\nTypes of socialism\n",
    "```\n",
    "\n",
    "where `\\n\\n` denotes a new section on the page, and the last section of the page is a list of categories (in the above example, the categories are Anti-capitalism, Anti-fascism, ... Types of socialism).\n",
    "\n",
    "In Pandas, we can create a new column called \"categories\" by using the following operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the text column, grab everything after the final occurence of \"\\n\\n\"\n",
    "df[\"categories\"] = df[\"text\"].str.rsplit(\"\\n\\n\", n=1).str[-1].str.strip().str.split(\"\\n\")\n",
    "\n",
    "# Add the title of the page to the list of categories\n",
    "df[\"categories\"] = df.apply(\n",
    "    lambda row: [row[\"title\"]] + row[\"categories\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac76d20",
   "metadata": {},
   "source": [
    "With this in mind, implement a stage in Curator which adds a \"categories\" column to the entire dataset. Here is a skeleton to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from nemo_curator.stages.base import ProcessingStage\n",
    "from nemo_curator.tasks import DocumentBatch\n",
    "\n",
    "@dataclass\n",
    "class CategoriesAdder(ProcessingStage[DocumentBatch, DocumentBatch]):\n",
    "    \"\"\"\n",
    "    Adds a \"categories\" column to the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    text_field = \"text\"\n",
    "    title_field = \"title\"\n",
    "    categories_field = \"categories\"  # list[str]\n",
    "\n",
    "    def inputs(self):\n",
    "        return [[\"data\"], [self.text_field, self.title_field]]\n",
    "\n",
    "    def outputs(self):\n",
    "        return [[\"data\"], [self.categories_field]]\n",
    "\n",
    "    def process(self, batch: DocumentBatch) -> DocumentBatch:\n",
    "        # implement this function\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.text.io.reader import JsonlReader\n",
    "\n",
    "# Create a pipeline and add stages to it\n",
    "pipeline = Pipeline(\"wiki_categories_pipeline\")\n",
    "\n",
    "jsonl_reader = JsonlReader(wiki_data_dir)\n",
    "pipeline.add_stage(jsonl_reader)\n",
    "\n",
    "categories_adder = CategoriesAdder()\n",
    "pipeline.add_stage(categories_adder)\n",
    "\n",
    "wiki_categories_dir = \"./wiki_categories_data\"\n",
    "jsonl_writer = JsonlWriter(wiki_categories_dir)\n",
    "pipeline.add_stage(jsonl_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674495e9",
   "metadata": {},
   "source": [
    "Feel free to do some exploratory analyses of the categories of data available. Include your explorations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c14a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploratory analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29742246",
   "metadata": {},
   "source": [
    "## Implement a Filter [10 points]\n",
    "\n",
    "Next, choose a topic of interest and implement the following filter. Aim for 1,000 - 10,000 or more documents (rows) in your `wiki_domain_dir`.\n",
    "\n",
    "Note in the below code, we expect `domains` to be a list of strings. This is intended to help keep more documents matching your targeted area. For example, if your targeted domain is political theory, then an appropriate usage might be `domains=[\"politics\", \"political theory\", \"conservatism\", \"liberalism\", ...]` to catch as many relevant matches as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c451a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.text.filters.doc_filter import DocumentFilter\n",
    "\n",
    "class DomainFilter(DocumentFilter):\n",
    "    \"\"\"\n",
    "    Throw away documents whose categories column does not contain the given substring(s)\n",
    "\n",
    "    Advice:\n",
    "    - Check if any of the domains are in the categories, even as a substring\n",
    "    - This function should not be case-sensitive\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, domains: list[str]):\n",
    "        super().__init__()\n",
    "        self._domains = domains\n",
    "        self._name = \"domain_filter\"\n",
    "\n",
    "    def score_document(self, categories: list[str]) -> list[str]:\n",
    "        return categories  # do not modify this function\n",
    "\n",
    "    def keep_document(self, categories: list[str]) -> bool:\n",
    "        # implement this function\n",
    "        # keep_document should return True if the document contains any of the domains and False otherwise\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5066fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.stages.text.modules import Filter\n",
    "\n",
    "# Create a pipeline and add stages to it\n",
    "pipeline = Pipeline(\"wiki_domain_pipeline\")\n",
    "\n",
    "jsonl_reader = JsonlReader(wiki_categories_dir)\n",
    "pipeline.add_stage(jsonl_reader)\n",
    "\n",
    "domains = []  # replace with your list of domains\n",
    "domain_filter = DomainFilter(domains=domains)\n",
    "pipeline.add_stage(Filter(domain_filter, filter_field=\"categories\"))\n",
    "\n",
    "wiki_domain_dir = \"./wiki_domain_data\"\n",
    "jsonl_writer = JsonlWriter(wiki_domain_dir)\n",
    "pipeline.add_stage(jsonl_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614de38",
   "metadata": {},
   "source": [
    "You can use the following bash command to count the number documents within your `wiki_domain_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ./wiki_domain_data/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1504b1",
   "metadata": {},
   "source": [
    "Feel free to do some exploratory analyses of the categories of data available. Include your explorations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291995b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploratory analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac5ed0",
   "metadata": {},
   "source": [
    "## Implement a Pipeline [30 points]\n",
    "\n",
    "Create and run a data curation pipeline. Explain your reasoning for each stage included and how you expect it to be relevant to your target domain and/or findings from your exploratory analysis.\n",
    "\n",
    "Some options include:\n",
    "- Heuristic filters: using `ScoreFilter` and/or `Filter` as the stage(s), with an existing or custom `DocumentFilter` as the parameter(s)\n",
    "- Modifiers: using `Modify` as the stage(s), with an existing or custom `DocumentModifier` as the parameter(s)\n",
    "- Deduplication: exact, fuzzy, and/or semantic deduplication workflow(s)\n",
    "- Quality classifiers: determining educational value using a FineWeb-Edu classifier, etc.\n",
    "\n",
    "Please note that since deduplication is not a map-style operation, you do **not** add it to a `Pipeline`. Instead, deduplication is initialized as a **workflow** (e.g., `workflow = TextSemanticDeduplicationWorkflow(...)`) and run with `workflow.run()`. See the existing [tutorials](https://github.com/NVIDIA-NeMo/Curator/tree/main/tutorials/text/deduplication) and/or NeMo Curator documentation for more help.\n",
    "\n",
    "You are encouraged to implement own own custom filters and/or stages in Curator! You are also encouraged to look at the number of dropped documents or other interesting statistics per stage.\n",
    "\n",
    "Save your final curated dataset in a directory called `wiki_curated_domain_data/`.\n",
    "\n",
    "### Scoring\n",
    "\n",
    "- A deduplication workflow is required [10 points]\n",
    "- Additional stages (not including read/write) are 5 points each (up to 15 points)\n",
    "- Justification for each stage is required [5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = Pipeline(\"wiki_curation_pipeline\")\n",
    "\n",
    "jsonl_reader = JsonlReader(wiki_domain_dir)\n",
    "pipeline.add_stage(jsonl_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e21529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stages here\n",
    "# justify your choices either in the comments or in a markdown cell\n",
    "# remember that the order of the stages matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_data_dir = \"./wiki_curated_domain_data\"\n",
    "jsonl_writer = JsonlWriter(curated_data_dir)\n",
    "pipeline.add_stage(jsonl_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b44b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fce921",
   "metadata": {},
   "source": [
    "## Evaluate the Data [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c437d60",
   "metadata": {},
   "source": [
    "In this section, we will compare the non-curated data within `wiki_domain_data/` with the curated data within `wiki_curated_domain_data/`.\n",
    "\n",
    "The best way to compare the effectiveness of curated versus non-curated data is to train separate models with each dataset and evaluate the resulting models themselves. However, in the interest of time, we will use **perplexity** as our evaluation metric.\n",
    "\n",
    "Perplexity is colloquially referred to how \"confusing\" a piece of text is to an LLM. A lower perplexity score indicates that the model is less \"perplexed,\" meaning it is more confident and accurate in predicting the next token in a sequence. This means that we would expect the perplexity of the curated data to be lower than the perplexity of the non-curated data, but maybe that will not be the case for your datasets. If this happens, it is okay. You should reason about the results with respect to your chosen domain.\n",
    "\n",
    "The cell below includes a basic function for calculating the perplexity of a text using the `gpt2` model. Calculate and plot the perplexities of the non-curated versus curated datasets.\n",
    "\n",
    "Consider the following:\n",
    "- The provided function is slow for computing the perplexities of hundreds or thousands of documents. Convert it into a Curator stage.\n",
    "- The `gpt2` model is a nice lightweight and generic model to use here. Evaluation using the `gpt2` model is required. Additionally, you are encouraged to try it out with different models depending on your chosen domain (e.g., if your domain is medicine, consider evaluating with a model specifically intended for medical and clinical text).\n",
    "\n",
    "### Scoring\n",
    "\n",
    "- Conversion to Curator stage [10 points]\n",
    "- Plots for the perplexities of the non-curated versus curated data [5 points]\n",
    "- Extension to domain-specific models and/or metrics, with a comprehensive analysis per method [5 bonus points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ee091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this cell\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_name = \"gpt2\"  # small, fast for experiments\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def compute_perplexity(texts):\n",
    "    \"\"\"\n",
    "    texts: list of strings\n",
    "    returns: list of perplexity scores\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize\n",
    "        encodings = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "        input_ids = encodings.input_ids.to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            # Cross-entropy loss per token\n",
    "            loss = outputs.loss\n",
    "        # Perplexity = exp(loss)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    return perplexities\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"This is a well-written coherent sentence.\",\n",
    "    \"Ths txt has typos and is hard to read.\"\n",
    "]\n",
    "\n",
    "perplexities = compute_perplexity(texts)\n",
    "for t, p in zip(texts, perplexities):\n",
    "    print(f\"Perplexity: {p:.2f} | Text: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a curator stage here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexities of the non-curated and curated datasets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot perplexities here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7a076",
   "metadata": {},
   "source": [
    "You are invited to experiment with other methods to evaluate the non-curated versus curated datasets. Include them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional explorations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e4ed0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Reason about the assignment and/or your findings here.\n",
    "\n",
    "Once you are done with any Curator-related pipelines, stop the Ray client with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_client.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curator_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
