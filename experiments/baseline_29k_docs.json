{
  "experiment_id": "baseline_29k_docs",
  "timestamp": "2026-02-28T23:53:22.535184",
  "description": "Baseline pre-training run: full hybrid dataset (24K real + 5K synthetic)",
  "data": {
    "path": "/home/jason/ee194-a2/partb_data_designer_exports/partb_hybrid_training_data.jsonl",
    "num_documents": 29074,
    "real_documents": 24000,
    "synthetic_documents": 5000,
    "total_tokens": 26172855,
    "sequences_2048": 12779
  },
  "tokenizer": {
    "type": "byte-level BPE (HuggingFace tokenizers)",
    "vocab_size": 16384,
    "trained_on": "hybrid pre-training corpus",
    "dir": "tokenizer_16k"
  },
  "model": {
    "family": "Nemotron-Nano-V3 (dense, scaled down)",
    "vocab_size": 16384,
    "hidden_size": 128,
    "num_layers": 8,
    "num_attention_heads": 8,
    "num_kv_heads": 2,
    "intermediate_size": 512,
    "max_position_embeddings": 2048,
    "total_params_tied": 3999872,
    "embedding_params": 2097152,
    "non_embedding_params": 1902720
  },
  "training": {
    "epochs": 3,
    "batch_size_per_gpu": 8,
    "gradient_accumulation_steps": 1,
    "num_gpus": 8,
    "global_batch_size": 64,
    "learning_rate": 0.0003,
    "weight_decay": 0.1,
    "warmup_fraction": 0.05,
    "dtype": "bfloat16",
    "optimizer": "AdamW (beta1=0.9, beta2=0.95)",
    "schedule": "linear warmup + cosine decay",
    "best_train_loss": 6.453744809232165,
    "best_epoch": 3,
    "total_steps": 597,
    "tokens_per_param_ratio": 19.6
  },
  "eval_results": {
    "hellaswag_acc_norm": 0.25721967735510853,
    "piqa_acc": 0.5179542981501633,
    "winogrande_acc": 0.5043409629044988,
    "boolq_acc": 0.3782874617737003,
    "arc_easy_acc_norm": 0.2718855218855219,
    "arc_challenge_acc_norm": 0.25597269624573377,
    "sciq_acc_norm": 0.234,
    "wikitext_word_ppl": 4353.833128796097,
    "wikitext_bits_per_byte": 2.260529589550235
  },
  "eval_results_full": {
    "arc_challenge": {
      "alias": "arc_challenge",
      "acc,none": 0.19880546075085323,
      "acc_stderr,none": 0.011662850198175543,
      "acc_norm,none": 0.25597269624573377,
      "acc_norm_stderr,none": 0.01275301324124452
    },
    "arc_easy": {
      "alias": "arc_easy",
      "acc,none": 0.2676767676767677,
      "acc_stderr,none": 0.009085000147099356,
      "acc_norm,none": 0.2718855218855219,
      "acc_norm_stderr,none": 0.009129795867310497
    },
    "boolq": {
      "alias": "boolq",
      "acc,none": 0.3782874617737003,
      "acc_stderr,none": 0.008482001133931
    },
    "hellaswag": {
      "alias": "hellaswag",
      "acc,none": 0.2597092212706632,
      "acc_stderr,none": 0.004375788991216852,
      "acc_norm,none": 0.25721967735510853,
      "acc_norm_stderr,none": 0.004362081806560223
    },
    "lambada_openai": {
      "alias": "lambada_openai",
      "perplexity,none": 4712109.928531486,
      "perplexity_stderr,none": 378954.01817056513,
      "acc,none": 1.0,
      "acc_stderr,none": 0.0
    },
    "piqa": {
      "alias": "piqa",
      "acc,none": 0.5179542981501633,
      "acc_stderr,none": 0.011658300623287154,
      "acc_norm,none": 0.500544069640914,
      "acc_norm_stderr,none": 0.011665817258899178
    },
    "sciq": {
      "alias": "sciq",
      "acc,none": 0.209,
      "acc_stderr,none": 0.012864077288499344,
      "acc_norm,none": 0.234,
      "acc_norm_stderr,none": 0.013394902889660014
    },
    "wikitext": {
      "alias": "wikitext",
      "word_perplexity,none": 4353.833128796097,
      "word_perplexity_stderr,none": "N/A",
      "byte_perplexity,none": 4.791673439949941,
      "byte_perplexity_stderr,none": "N/A",
      "bits_per_byte,none": 2.260529589550235,
      "bits_per_byte_stderr,none": "N/A"
    },
    "winogrande": {
      "alias": "winogrande",
      "acc,none": 0.5043409629044988,
      "acc_stderr,none": 0.014051956064076903
    }
  }
}